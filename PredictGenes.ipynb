{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These jupyter can reproduced the results of figure2 in our papers.\n",
    "you can use these functions we defined to evaluate the performance of each method with 10-X cross-validation.\n",
    "If you want to use each method to analyse youe own data, please see the Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import time as tm\n",
    "from functools import partial\n",
    "import scipy.stats as st\n",
    "from scipy.stats import wasserstein_distance\n",
    "import scipy.stats\n",
    "import copy\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import matplotlib as mpl \n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "import h5py\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "from torch.nn.functional import softmax, cosine_similarity, sigmoid\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we defined the funtions of each integration methods,\n",
    "Please note :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want use normlized spatial and scRNA-seq data to integration,\n",
    "\n",
    "For gimVI, SpaGE, Tangram, novoSpaRc and SpaOTsc, you must use normlized data as input, while,  \n",
    "\n",
    "For Seurat and LIGER, you can use parameters norm == 'norm' directly with raw count file as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GenePrediction:\n",
    "    def __init__(self, RNA_path, Spatial_path, location_path, count_path = None, device = None, train_list = None, test_list = None, norm = 'count', outdir = None):\n",
    "        \"\"\"\n",
    "        @author: wen zhang\n",
    "        This function integrates two single-cell datasets, spatial and scRNA-seq, \n",
    "        and predictes the expression of the spatially unmeasured genes from the scRNA-seq data.\n",
    "        \n",
    "        Parameters\n",
    "        -------\n",
    "        RNA_path : str\n",
    "            scRNA-seq data count file with Tab-delimited (cells X genes).\n",
    "        Spatial_path : str\n",
    "            spatial count data file with Tab-delimited, please note that the file has no index.\n",
    "        location_path : str\n",
    "            spatial spot coordinate file name with Tab-delimited, please note that the file has no index.\n",
    "        count_path : str\n",
    "            Option,  you must prepared this file when you want to use Tangram (tangram_seq_impute)to prediction gene spatial distribution.\n",
    "        device : str\n",
    "            Option,  [None,'GPU'], defaults to None\n",
    "        train_list : list\n",
    "            genes for integrations, you can support more than one train list.\n",
    "        test_list : list\n",
    "            genes for prediction, you can support more than one test list.\n",
    "        norm : str\n",
    "            Option,  ['count','norm'], defaults to count. if norm, Seurat and LIGER\n",
    "            will normlize the  spatial and scRNA-seq data before intergration.\n",
    "        outdir : str\n",
    "            result file stored direction    \n",
    "        \"\"\"\n",
    "        self.RNA_file = RNA_path\n",
    "        self.Spatial_file = Spatial_path\n",
    "        self.locations = np.loadtxt(location_path, skiprows=1)\n",
    "        self.train_list = train_list\n",
    "        self.test_list = test_list\n",
    "        self.RNA_data_adata = sc.read(RNA_path, sep = \"\\t\",first_column_names=True).T\n",
    "        self.Spatial_data_adata = sc.read(Spatial_path, sep = \"\\t\")\n",
    "        self.device = device\n",
    "        self.norm = norm\n",
    "        print ('Please note you are using ' + self.norm + ' expression matrix to predict')\n",
    "        if count_path != None:\n",
    "            self.count =pd.read_table(count_path,sep='\\t').astype(int)\n",
    "            self.count[self.count.cell_counts==0]=1\n",
    "        self.outdir = outdir\n",
    "        \n",
    "        \n",
    "    def SpaGE_impute(self,args):\n",
    "        sys.path.append(\"FigureData/SpaGE-master/\")\n",
    "        from SpaGE.main import SpaGE\n",
    "        RNA_data = pd.read_table(self.RNA_file,header=0,index_col = 0)\n",
    "        Spatial_data = pd.read_table(self.Spatial_file,sep='\\t',header=0)\n",
    "        RNA_data = RNA_data.loc[(RNA_data.sum(axis=1) != 0)]\n",
    "        RNA_data = RNA_data.loc[(RNA_data.var(axis=1) != 0)]\n",
    "        train_list, test_list = args\n",
    "        predict = test_list\n",
    "        feature = train_list\n",
    "\n",
    "        if (len(feature)) < 50: \n",
    "            pv = int(len(feature)-3)\n",
    "        else:\n",
    "            pv = 50\n",
    "        Spatial = Spatial_data[feature]\n",
    "        Img_Genes = SpaGE(Spatial,RNA_data.T,n_pv=pv,genes_to_predict = predict)\n",
    "        result = Img_Genes[predict]\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def gimVI_impute(self,args):\n",
    "        import scvi\n",
    "        import scanpy as sc\n",
    "        from scvi.model import GIMVI\n",
    "        Spatial_data_adata = self.Spatial_data_adata\n",
    "        RNA_data_adata = self.RNA_data_adata\n",
    "        train_list, test_list = args\n",
    "        Genes  = list(Spatial_data_adata.var_names)\n",
    "        rand_test_gene_idx = [Genes.index(x) for x in test_list]\n",
    "        n_genes = len(Genes)\n",
    "        rand_train_gene_idx = sorted(set(range(n_genes)) - set(rand_test_gene_idx))\n",
    "        rand_train_genes = np.array(Genes)[rand_train_gene_idx]\n",
    "        rand_test_genes = np.array(Genes)[rand_test_gene_idx]\n",
    "        spatial_data_partial = Spatial_data_adata[:, rand_train_genes]\n",
    "        sc.pp.filter_cells(spatial_data_partial, min_counts= 0)\n",
    "        \n",
    "        seq_data = copy.deepcopy(RNA_data_adata)\n",
    "        \n",
    "        seq_data = seq_data[:, Genes]\n",
    "        sc.pp.filter_cells(seq_data, min_counts = 0)\n",
    "        \n",
    "        scvi.data.setup_anndata(spatial_data_partial)\n",
    "        scvi.data.setup_anndata(seq_data)\n",
    "        \n",
    "        model = GIMVI(seq_data, spatial_data_partial)\n",
    "        model.train(200)\n",
    "        \n",
    "        _, imputation = model.get_imputed_values(normalized=False)\n",
    "        imputed = imputation[:, rand_test_gene_idx]\n",
    "        result = pd.DataFrame(imputed, columns=rand_test_genes)\n",
    "        return result\n",
    "    \n",
    "    def novoSpaRc_impute(self, args):\n",
    "        import novosparc as nc\n",
    "        RNA_data = pd.read_table(self.RNA_file,header=0,index_col = 0)\n",
    "        Spatial_data = pd.read_table(self.Spatial_file,sep='\\t',header=0)\n",
    "        train_list, test_list = args\n",
    "        gene_names = np.array(RNA_data.index.values)\n",
    "        dge = RNA_data.values\n",
    "        dge = dge.T\n",
    "        num_cells = dge.shape[0]\n",
    "        print ('number of cells and genes in the matrix:', dge.shape)\n",
    "    \n",
    "        hvg = np.argsort(np.divide(np.var(dge,axis=0),np.mean(dge,axis=0)+0.0001))\n",
    "        dge_hvg = dge[:,hvg[-2000:]]\n",
    "        \n",
    "        num_locations = self.locations.shape[0]\n",
    "    \n",
    "        p_location, p_expression = nc.rc.create_space_distributions(num_locations, num_cells)\n",
    "        cost_expression, cost_locations = nc.rc.setup_for_OT_reconstruction(dge_hvg,self.locations,num_neighbors_source = 5,num_neighbors_target = 5)\n",
    "        \n",
    "        insitu_matrix = np.array(Spatial_data[train_list])\n",
    "        insitu_genes = np.array(Spatial_data[train_list].columns)\n",
    "        test_genes = np.array(test_list)\n",
    "        test_matrix = np.array(Spatial_data[test_list])\n",
    "        print (test_genes)\n",
    "        print (insitu_genes)\n",
    "        \n",
    "        markers_in_sc = np.array([], dtype='int')\n",
    "        for marker in insitu_genes:\n",
    "            marker_index = np.where(gene_names == marker)[0]\n",
    "            if len(marker_index) > 0:\n",
    "                markers_in_sc = np.append(markers_in_sc, marker_index[0])\n",
    "        \n",
    "        cost_marker_genes = cdist(dge[:, markers_in_sc]/np.amax(dge[:, markers_in_sc]),insitu_matrix/np.amax(insitu_matrix))\n",
    "        alpha_linear = 0.5\n",
    "        gw = nc.rc._GWadjusted.gromov_wasserstein_adjusted_norm(cost_marker_genes, cost_expression, cost_locations,alpha_linear, p_expression, p_location,'square_loss', epsilon=5e-3, verbose=True)\n",
    "        sdge = np.dot(dge.T, gw)\n",
    "        imputed = pd.DataFrame(sdge,index=RNA_data.index)\n",
    "        result = imputed.loc[test_genes]\n",
    "        result = result.T\n",
    "        return result\n",
    "    \n",
    "    def SpaOTsc_impute(self, args):\n",
    "        from spaotsc import SpaOTsc\n",
    "        RNA_data = pd.read_table(self.RNA_file,header=0,index_col = 0)\n",
    "        Spatial_data = pd.read_table(self.Spatial_file,sep='\\t',header=0)\n",
    "        train_list, test_list = args\n",
    "        df_sc = RNA_data.T\n",
    "        df_IS = Spatial_data\n",
    "        pts = self.locations\n",
    "        is_dmat = distance_matrix(pts, pts)\n",
    "        \n",
    "            \n",
    "        df_is=df_IS.loc[:,train_list]\n",
    "        \n",
    "        gene_is=df_is.columns.tolist()\n",
    "        gene_sc=df_sc.columns.tolist()\n",
    "        gene_overloap=list(set(gene_is).intersection(gene_sc))\n",
    "        a=df_is[gene_overloap]\n",
    "        b=df_sc[gene_overloap]\n",
    "        \n",
    "        \n",
    "        rho, pval = stats.spearmanr(a, b,axis=1)\n",
    "        rho[np.isnan(rho)]=0\n",
    "        mcc=rho[-(len(df_sc)):,0:len(df_is)]\n",
    "        C = np.exp(1-mcc) \n",
    "\n",
    "        issc = SpaOTsc.spatial_sc(sc_data=df_sc, is_data=df_is, is_dmat = is_dmat)\n",
    "\n",
    "        issc.transport_plan(C**2, alpha=0, rho=1.0, epsilon=1.0, cor_matrix=mcc, scaling=False)\n",
    "        gamma = issc.gamma_mapping\n",
    "        for j in range(gamma.shape[1]):\n",
    "            gamma[:,j] = gamma[:,j]/np.sum(gamma[:,j])\n",
    "        X_pred = np.matmul(gamma.T, np.array(issc.sc_data.values))\n",
    "\n",
    "        result = pd.DataFrame(data=X_pred, columns=issc.sc_data.columns.values)\n",
    "        test_genes = test_list\n",
    "        result = result.loc[:,test_genes]  \n",
    "        return result\n",
    "    \n",
    "    def Tangram_impute_image(self, args):\n",
    "        sys.path.append(\"FigureData/Tangram-master/\")\n",
    "        import mapping.utils\n",
    "        import mapping.mapping_optimizer\n",
    "        import mapping.plot_utils\n",
    "        train_list, test_list = args\n",
    "        RNA_data = pd.read_table(self.RNA_file,header=0,index_col = 0).T\n",
    "        adata= sc.AnnData(RNA_data)\n",
    "        Spatial_data = pd.read_table(self.Spatial_file,sep='\\t',header=0)\n",
    "        device = self.device\n",
    "        if self.device == 'GPU':\n",
    "            device = torch.device('cuda:0')\n",
    "        hyperparm = {'lambda_d' : 1, 'lambda_g1' : 1, 'lambda_g2' : 0, 'lambda_r' : 0,\n",
    "                'lambda_count' : 1, 'lambda_f_reg' : 1}\n",
    "        learning_rate = 0.1\n",
    "        num_epochs = 1000\n",
    "        \n",
    "        gene_diff = train_list\n",
    "        spatial_data = Spatial_data[gene_diff]\n",
    "        space_data= sc.AnnData(spatial_data)\n",
    "        \n",
    "        S = np.array(adata[:, gene_diff] .X) \n",
    "        G = np.array(space_data.X) \n",
    "        d = np.full(G.shape[0], 1/G.shape[0])  \n",
    "        S = np.log(1+S)\n",
    "        mapper = mapping.mapping_optimizer.MapperConstrained(\n",
    "            S=S, G=G, d=d, device=device, **hyperparm, target_count=G.shape[0])\n",
    "        output, F_out = mapper.train(learning_rate=learning_rate, num_epochs=num_epochs)\n",
    "        pre_gene = np.dot(adata[:, test_list].X.T, output)\n",
    "        pre_gene =pd.DataFrame(pre_gene,index=test_list,columns=space_data.obs_names).T\n",
    "        \n",
    "        return pre_gene\n",
    "    \n",
    "    def Tangram_impute_seq(self, args):\n",
    "        if self.device == 'GPU':\n",
    "            device = torch.device('cuda:0')\n",
    "        train_list, test_list = args\n",
    "        RNA_data = pd.read_table(self.RNA_file,header=0,index_col = 0).T\n",
    "        adata= sc.AnnData(RNA_data)\n",
    "        Spatial_data = pd.read_table(self.Spatial_file,sep='\\t',header=0)\n",
    "        device = self.device\n",
    "        hyperparm = {'lambda_d' : 1, 'lambda_g1' : 1, 'lambda_g2' : 0, 'lambda_r' : 0,\n",
    "                'lambda_count' : 1, 'lambda_f_reg' : 1}\n",
    "        learning_rate = 0.1\n",
    "        num_epochs = 6000\n",
    "        \n",
    "        gene_diff = train_list\n",
    "        spatial_data = Spatial_data[gene_diff]\n",
    "        space_data = sc.AnnData(spatial_data)\n",
    "        space_data.obs['cell_count'] = self.count.cell_counts.values\n",
    "        \n",
    "        S = np.array(adata[:, gene_diff].X) \n",
    "        G = np.array(space_data.X) \n",
    "        d = np.array(space_data.obs.cell_count)/space_data.obs.cell_count.sum() \n",
    "        mapper = mapping.mapping_optimizer.MapperConstrained(\n",
    "        S=S, G=G, d=d, device=device, **hyperparm, target_count = space_data.obs.cell_count.sum())\n",
    "        output, F_out = mapper.train(learning_rate=learning_rate, num_epochs=num_epochs)\n",
    "        pre_gene = np.dot(adata[:, test_list].X.T, output)\n",
    "        pre_gene =pd.DataFrame(pre_gene,index=test_list,columns=space_data.obs_names).T\n",
    "        \n",
    "        return pre_gene\n",
    "\n",
    "    def pool(self, need_tools):\n",
    "        if \"SpaGE\" in need_tools:\n",
    "            \n",
    "            with multiprocessing.Pool(10) as pool:\n",
    "                result_SpaGE = pd.concat(pool.map(self.SpaGE_impute, iterable=zip(self.train_list, self.test_list)),axis=1) \n",
    "                if not os.path.exists(self.outdir):\n",
    "                    os.mkdir(self.outdir)\n",
    "                result_SpaGE.to_csv(self.outdir + \"/result_SpaGE.csv\",header=1, index=1)\n",
    "                \n",
    "        if \"gimVI\" in need_tools:\n",
    "            with multiprocessing.Pool(10) as pool:\n",
    "                result_GimVI = pd.concat(pool.map(self.gimVI_impute, iterable=zip(self.train_list, self.test_list)),axis=1) \n",
    "                if not os.path.exists(self.outdir):\n",
    "                    os.mkdir(self.outdir)\n",
    "                result_GimVI.to_csv(self.outdir + \"result_gimVI.csv\",header=1, index=1)\n",
    "                \n",
    "        if \"novoSpaRc\" in need_tools:\n",
    "            \n",
    "            with multiprocessing.Pool(10) as pool:\n",
    "                result_Novosparc = pd.concat(pool.map(self.novoSpaRc_impute, iterable=zip(self.train_list, self.test_list)),axis=1) \n",
    "                if not os.path.exists(self.outdir):\n",
    "                    os.mkdir(self.outdir)\n",
    "                result_Novosparc.to_csv(self.outdir + \"/result_novoSpaRc.csv\",header=1, index=1)\n",
    "                \n",
    "        if \"SpaOTsc\" in need_tools:\n",
    "            \n",
    "            with multiprocessing.Pool(10) as pool:\n",
    "                result_Spaotsc = pd.concat(pool.map(self.SpaOTsc_impute, iterable=zip(self.train_list, self.test_list)),axis=1) \n",
    "                if not os.path.exists(self.outdir):\n",
    "                    os.mkdir(self.outdir)\n",
    "                result_Spaotsc.to_csv(self.outdir + \"/result_SpaoTSc.csv\",header=1, index=1)\n",
    "                \n",
    "        if \"Tangram_image\" in need_tools:\n",
    "            \n",
    "            with multiprocessing.Pool(10) as pool:\n",
    "                result_Tangram_image = pd.concat(pool.map(self.Tangram_impute_image, iterable=zip(self.train_list, self.test_list)),axis=1) \n",
    "                if not os.path.exists(self.outdir):\n",
    "                    os.mkdir(self.outdir)\n",
    "                result_Tangram_image.to_csv(self.outdir + \"/result_Tangram_image.csv\",header=1, index=1)\n",
    "                \n",
    "        if \"Tangram_seq\" in need_tools:\n",
    "            \n",
    "            with multiprocessing.Pool(10) as pool:\n",
    "                result_Tangram_seq = pd.concat(pool.map(self.Tangram_impute_seq, iterable=zip(self.train_list, self.test_list)),axis=1) \n",
    "                if not os.path.exists(self.outdir):\n",
    "                    os.mkdir(self.outdir)\n",
    "                result_Tangram_seq.to_csv(self.outdir + \"result_Tangram_seq.csv\",header=1, index=1)\n",
    "\n",
    "        if 'LIGER' in need_tools:\n",
    "            RLiger = {}\n",
    "            for k in range(len(self.train_list)):\n",
    "                train = ','.join(self.train_list[k])\n",
    "                test = ','.join(self.test_list[k])\n",
    "                os.system('Rscript Benchmarking/Liger.r ' + self.RNA_file + ' ' + self.Spatial_file + ' ' + train + ' ' + test + ' ' + self.norm  + ' ' + self.outdir + '/Result_LIGER_' + str(k) + '.txt')\n",
    "\n",
    "        if 'Seurat' in need_tools:\n",
    "            RSeurat = {}\n",
    "            for k in range(len(self.train_list)):\n",
    "                train = ','.join(self.train_list[k])\n",
    "                test = ','.join(self.test_list[k])\n",
    "                os.system ('Rscript Benchmarking/Seurat.r ' + self.RNA_file + ' ' + self.Spatial_file + ' ' + train + ' ' + test + ' ' + self.norm + ' ' + self.outdir + '/Result_Seurat_' + str(k) + '.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for osmFISH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please note you are using count expression matrix to predict\n"
     ]
    }
   ],
   "source": [
    "PATH = 'FigureData/Figure2/Dataset2_osmFISH/Rawdata/'\n",
    "RNA_path = PATH + 'scRNA_count.txt'\n",
    "Spatial_path =  PATH + 'Insitu_count.txt'\n",
    "location_path = PATH + 'Locations.txt'\n",
    "Spatial_data = pd.read_table(Spatial_path,sep='\\t',header=0)\n",
    "train_list = (np.load(PATH + \"train_list.npy\", allow_pickle=True)).tolist()\n",
    "test_list = (np.load(PATH + \"test_list.npy\", allow_pickle=True)).tolist()\n",
    "outdir =  'FigureData/Figure2/Dataset2_osmFISH/'\n",
    "if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "test = GenePrediction(RNA_path, Spatial_path, location_path, train_list = train_list, test_list = test_list, outdir = outdir)\n",
    "Methods = ['SpaGE','novoSpaRc','SpaOTsc','Tangram_image','gimVI','Seurat','Liger']\n",
    "Methods = ['SpaGE']\n",
    "Result = test.pool(Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for seqFISH+ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please note you are using count expression matrix to predict\n"
     ]
    }
   ],
   "source": [
    "PATH = 'FigureData/Figure2/Dataset4_seqFISH+/Rawdata/'\n",
    "RNA_path = PATH + 'scRNA_count.txt'\n",
    "Spatial_path =  PATH + 'Insitu_count.txt'\n",
    "location_path = PATH + 'Locations.txt'\n",
    "Spatial_data = pd.read_table(Spatial_path,sep='\\t',header=0)\n",
    "train_list = (np.load(PATH + \"train_list.npy\", allow_pickle=True)).tolist()\n",
    "test_list = (np.load(PATH + \"test_list.npy\", allow_pickle=True)).tolist()\n",
    "outdir =  'FigureData/Figure2/Dataset4_seqFISH+/'\n",
    "if not os.path.exists(outdir):\n",
    "        os.mkdir(outdir)\n",
    "test = GenePrediction(RNA_path, Spatial_path, location_path, train_list = train_list, test_list = test_list, outdir = outdir)\n",
    "Methods = ['SpaGE','novoSpaRc','SpaOTsc','Tangram_image','gimVI','Seurat','Liger']\n",
    "Methods = ['SpaGE']\n",
    "Result = test.pool(Methods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.We combined the 10-X prediction results of LIGER or Seurat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_dir_name = '/Users/wenzhang/Desktop/SpatialBenchmarking/FigureData/Figure2/Dataset2_osmFISH/'\n",
    "all_files = os.listdir(read_dir_name)\n",
    "read_files = [x for x in all_files if x[:13]=='Result_LIGER_']\n",
    "R = None\n",
    "for F in read_files:\n",
    "    Files = read_dir_name + F\n",
    "    df = pd.read_csv(Files, sep = '\\t', index_col = 0)\n",
    "    if R is None:\n",
    "        R = df\n",
    "    else:\n",
    "        R = pd.concat([R,df])\n",
    "    Col = list(R.columns)\n",
    "R.columns = [(x.split('V')[1]) for x in Col]\n",
    "Index = [(int(x)-1) for x in R.columns]\n",
    "R.columns = Index\n",
    "Index = sorted(Index)\n",
    "R = R[Index].T  \n",
    "R.to_csv(read_dir_name + 'result_LIGER.csv',header=1, index=1)\n",
    "os.system('rm ' + read_dir_name + 'Result_LIGER_*txt')\n",
    "\n",
    "\n",
    "all_files = os.listdir(read_dir_name)\n",
    "read_files = [x for x in all_files if x[:14]=='Result_Seurat_']\n",
    "R = None\n",
    "for F in read_files:\n",
    "    Files = read_dir_name + F\n",
    "    df = pd.read_csv(Files, sep = '\\t', index_col = 0)\n",
    "    if R is None:\n",
    "        R = df\n",
    "    else:\n",
    "        R = pd.concat([R,df])\n",
    "    Col = list(R.columns)\n",
    "Index = [(int(x)-1) for x in R.columns]\n",
    "R.columns = Index\n",
    "Index = sorted(Index)\n",
    "R = R[Index].T  \n",
    "R.to_csv(read_dir_name + 'result_Seurat.csv',header=1, index=1)\n",
    "os.system('rm ' + read_dir_name + 'Result_Seurat_*txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_dir_name = '/Users/wenzhang/Desktop/SpatialBenchmarking/FigureData/Figure2/Dataset4_seqFISH+/'\n",
    "all_files = os.listdir(read_dir_name)\n",
    "read_files = [x for x in all_files if x[:13]=='Result_LIGER_']\n",
    "R = None\n",
    "for F in read_files:\n",
    "    Files = read_dir_name + F\n",
    "    df = pd.read_csv(Files, sep = '\\t', index_col = 0)\n",
    "    if R is None:\n",
    "        R = df\n",
    "    else:\n",
    "        R = pd.concat([R,df])\n",
    "    Col = list(R.columns)\n",
    "R.columns = [(x.split('V')[1]) for x in Col]\n",
    "Index = [(int(x)-1) for x in R.columns]\n",
    "R.columns = Index\n",
    "Index = sorted(Index)\n",
    "R = R[Index].T \n",
    "R.to_csv(read_dir_name + 'result_LIGER.csv',header=1, index=1)\n",
    "os.system('rm ' + read_dir_name + 'Result_LIGER_*txt')\n",
    "\n",
    "all_files = os.listdir(read_dir_name)\n",
    "read_files = [x for x in all_files if x[:14]=='Result_Seurat_']\n",
    "R = None\n",
    "for F in read_files:\n",
    "    Files = read_dir_name + F\n",
    "    df = pd.read_csv(Files, sep = '\\t', index_col = 0)\n",
    "    if R is None:\n",
    "        R = df\n",
    "    else:\n",
    "        R = pd.concat([R,df])\n",
    "    Col = list(R.columns)\n",
    "Index = [(int(x)-1) for x in R.columns]\n",
    "R.columns = Index\n",
    "Index = sorted(Index)\n",
    "R = R[Index].T \n",
    "R.to_csv(read_dir_name + 'result_Seurat.csv',header=1, index=1)\n",
    "os.system('rm ' + read_dir_name + 'Result_Seurat_*txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.We used four metrics to evaluate the performance of each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ssim(im1,im2,M):\n",
    "    assert len(im1.shape) == 2 and len(im2.shape) == 2\n",
    "    assert im1.shape == im2.shape\n",
    "    mu1 = im1.mean()\n",
    "    mu2 = im2.mean()\n",
    "    sigma1 = np.sqrt(((im1 - mu1) ** 2).mean())\n",
    "    sigma2 = np.sqrt(((im2 - mu2) ** 2).mean())\n",
    "    sigma12 = ((im1 - mu1) * (im2 - mu2)).mean()\n",
    "    k1, k2, L = 0.01, 0.03, M\n",
    "    C1 = (k1*L) ** 2\n",
    "    C2 = (k2*L) ** 2\n",
    "    C3 = C2/2\n",
    "    l12 = (2*mu1*mu2 + C1)/(mu1 ** 2 + mu2 ** 2 + C1)\n",
    "    c12 = (2*sigma1*sigma2 + C2)/(sigma1 ** 2 + sigma2 ** 2 + C2)\n",
    "    s12 = (sigma12 + C3)/(sigma1*sigma2 + C3)\n",
    "    ssim = l12 * c12 * s12\n",
    "    \n",
    "    return ssim\n",
    "\n",
    "def scale_max(df):\n",
    "    result = pd.DataFrame()\n",
    "    for label, content in df.items():\n",
    "        content = content/content.max()\n",
    "        result = pd.concat([result, content],axis=1).fillna(1e-20)\n",
    "    return result\n",
    "\n",
    "def scale_z_score(df):\n",
    "    result = pd.DataFrame()\n",
    "    for label, content in df.items():\n",
    "        content = stats.zscore(content)\n",
    "        content = pd.DataFrame(content,columns=[label])\n",
    "        result = pd.concat([result, content],axis=1).fillna(1e-20)\n",
    "    return result\n",
    "\n",
    "\n",
    "def scale_plus(df):\n",
    "    result = pd.DataFrame()\n",
    "    for label, content in df.items():\n",
    "        content = content/content.sum()\n",
    "        result = pd.concat([result,content],axis=1).fillna(1e-20)\n",
    "    return result\n",
    "\n",
    "class CalculateMeteics:\n",
    "    def __init__(self, raw_count_file, impute_count_path, tool, outdir, metric,Tools):\n",
    "        self.raw_count = pd.read_csv(raw_count_file, header=0, sep=\"\\t\")\n",
    "        self.impute_count = pd.read_csv(impute_count_path + 'result_' + tool + '.csv', header=0, index_col=0)\n",
    "        self.impute_count = self.impute_count.fillna(1e-20)\n",
    "        self.tool = tool\n",
    "        self.outdir = outdir\n",
    "        self.metric = metric\n",
    "        self.Tools = Tools\n",
    "        \n",
    "    def SSIM(self, raw, impute, scale = 'scale_max'):\n",
    "        if scale == 'scale_max':\n",
    "            raw = scale_max(raw)\n",
    "            impute = scale_max(impute)\n",
    "        else:\n",
    "            print ('Please note you do not scale data by scale max')\n",
    "        if raw.shape[1] == impute.shape[1]:\n",
    "            result = pd.DataFrame()\n",
    "            for label in raw.columns:\n",
    "                if label not in impute.columns:\n",
    "                    ssim = 0\n",
    "                else:\n",
    "                    raw_col =  raw.loc[:,label]\n",
    "                    impute_col = impute.loc[:,label]\n",
    "                \n",
    "                    M = [raw_col.max(),impute_col.max()][raw_col.max()>impute_col.max()]\n",
    "                    raw_col_2 = np.array(raw_col)\n",
    "                    raw_col_2 = raw_col_2.reshape(raw_col_2.shape[0],1)\n",
    "                \n",
    "                    impute_col_2 = np.array(impute_col)\n",
    "                    impute_col_2 = impute_col_2.reshape(impute_col_2.shape[0],1)\n",
    "                \n",
    "                    ssim = cal_ssim(raw_col_2,impute_col_2,M)\n",
    "                \n",
    "                ssim_df = pd.DataFrame(ssim, index=[\"SSIM\"],columns=[label])\n",
    "                result = pd.concat([result, ssim_df],axis=1)\n",
    "            return result\n",
    "        else:\n",
    "            print(\"columns error\")\n",
    "            \n",
    "    def PCC(self, raw, impute, scale = None):\n",
    "        if raw.shape[1] == impute.shape[1]:\n",
    "            result = pd.DataFrame()\n",
    "            for label in raw.columns:\n",
    "                if label not in impute.columns:\n",
    "                     pearsonr = 0\n",
    "                else:\n",
    "                    raw_col =  raw.loc[:,label]\n",
    "                    impute_col = impute.loc[:,label]\n",
    "                    pearsonr, _ = st.pearsonr(raw_col,impute_col)\n",
    "                pearson_df = pd.DataFrame(pearsonr, index=[\"PCC\"],columns=[label])\n",
    "                result = pd.concat([result, pearson_df],axis=1)\n",
    "            return result\n",
    "        \n",
    "    def JS(self, raw, impute, scale = 'scale_plus'):\n",
    "        if scale == 'scale_plus':\n",
    "            raw = scale_plus(raw)\n",
    "            impute = scale_plus(impute)\n",
    "        else:\n",
    "            print ('Please note you do not scale data by plus')    \n",
    "        if raw.shape[1] == impute.shape[1]:\n",
    "            result = pd.DataFrame()\n",
    "            for label in raw.columns:\n",
    "                if label not in impute.columns:\n",
    "                    JS = 1\n",
    "                else:\n",
    "                    raw_col =  raw.loc[:,label]\n",
    "                    impute_col = impute.loc[:,label]\n",
    "                \n",
    "                    M = (raw_col + impute_col)/2\n",
    "                    JS = 0.5*st.entropy(raw_col,M)+0.5*st.entropy(impute_col,M)\n",
    "                JS_df = pd.DataFrame(JS, index=[\"JS\"],columns=[label])\n",
    "                result = pd.concat([result, JS_df],axis=1)\n",
    "            return result\n",
    "        \n",
    "    def RMSE(self, raw, impute, scale = 'zscore'):\n",
    "        if scale == 'zscore':\n",
    "            raw = scale_z_score(raw)\n",
    "            impute = scale_plus(impute)\n",
    "        else:\n",
    "            print ('Please note you do not scale data by zscore')\n",
    "        if raw.shape[1] == impute.shape[1]:\n",
    "            result = pd.DataFrame()\n",
    "            for label in raw.columns:\n",
    "                if label not in impute.columns:\n",
    "                    RMSE = 1.5   \n",
    "                else:\n",
    "                    raw_col =  raw.loc[:,label]\n",
    "                    impute_col = impute.loc[:,label]\n",
    "                \n",
    "                    RMSE = np.sqrt(((raw_col - impute_col) ** 2).mean())\n",
    "                RMSE_df = pd.DataFrame(RMSE, index=[\"RMSE\"],columns=[label])\n",
    "                result = pd.concat([result, RMSE_df],axis=1)\n",
    "            return result\n",
    "                \n",
    "        \n",
    "    def compute_all(self):\n",
    "        raw = self.raw_count\n",
    "        impute = self.impute_count\n",
    "        tool = self.tool\n",
    "        outdir = self.outdir\n",
    "        SSIM = self.SSIM(raw,impute)\n",
    "        Pearson = self.PCC(raw, impute)\n",
    "        JS = self.JS(raw, impute)\n",
    "        RMSE = self.RMSE(raw, impute)\n",
    "        \n",
    "        result_all = pd.concat([Pearson, SSIM, RMSE, JS],axis=0)\n",
    "        \n",
    "        if not os.path.exists(outdir):\n",
    "            os.mkdir(outdir)\n",
    "        result_all.T.to_csv(outdir + tool +\"_metrics.txt\",sep='\\t',header=1, index=1)\n",
    "        self.accuracy = result_all\n",
    "        return result_all\n",
    "        \n",
    "def plot_boxplot(PATH,metric,Tools,outdir):\n",
    "    font = {'family':'DejaVu Sans','weight':'normal','size':15}\n",
    "    plt.figure(figsize=(18,16), dpi= 80)\n",
    "    result = pd.DataFrame()\n",
    "    metrics = metric\n",
    "    tools = Tools\n",
    "    for tool in tools:\n",
    "        result_metrics = pd.read_csv(PATH + tool + '_metrics.txt',sep = '\\t',index_col = 0)\n",
    "        result_metrics['tool'] = tool\n",
    "        result = pd.concat([result, result_metrics],axis=0) \n",
    "    n = 221\n",
    "    for method in metrics:\n",
    "        ax1 = plt.subplot(n)\n",
    "        ax1 = sns.boxplot(x = 'tool', y = method, data = result, fliersize=1,showcaps = True,whis = 0.5 ,showfliers = False)\n",
    "        ax1.set_xlabel(method)\n",
    "        #ax1.set_ylabel(tool)\n",
    "        ax1.spines['top'].set_visible(False)\n",
    "        ax1.spines['right'].set_visible(False)\n",
    "        n = n + 1 \n",
    "        plt.yticks([])\n",
    "    OutPdf = outdir\n",
    "    if not os.path.exists(OutPdf):\n",
    "        os.mkdir(OutPdf)\n",
    "    plt.savefig(OutPdf + \"/Accuracy_metrics.pdf\")\n",
    "    plt.show()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "raw_count_file = '/Users/wenzhang/Desktop/backUp/osmFISH/Insitu_count.txt'\n",
    "impute_count_path = '/Users/wenzhang/Desktop/SpatialBenchmarking/FigureData/Figure2/Dataset2_osmFISH/'\n",
    "outdir = '/Users/wenzhang/Desktop/SpatialBenchmarking/FigureData/Figure2/Dataset2_osmFISH/Metrics/'\n",
    "metric = ['PCC','SSIM','RMSE','JS']\n",
    "Tools = ['gimVI','SpaGE','Tangram','Seurat','SpaOTsc','LIGER','novoSpaRc']\n",
    "for tool in Tools:\n",
    "    CM = CalculateMeteics(raw_count_file = raw_count_file, impute_count_path = impute_count_path, tool = tool, outdir = outdir, metric = metric, Tools = Tools)\n",
    "    CM.compute_all()\n",
    "PATH = '/Users/wenzhang/Desktop/SpatialBenchmarking/FigureData/Figure2/Dataset2_osmFISH/Metrics/'\n",
    "result = plot_boxplot(PATH,metric,Tools,outdir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
